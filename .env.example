# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (must be pulled first with: ollama pull <model>)
# Good options with tool-calling support:
#   qwen2.5:7b     - Fast, good tool use (recommended)
#   llama3.1:8b    - Meta's model, solid tool support
#   mistral:7b     - Good quality, fast
#   qwen2.5:14b    - Higher quality, needs more RAM
#   llama3.1:70b   - Best quality, needs significant RAM/GPU
OLLAMA_MODEL=qwen2.5:7b
